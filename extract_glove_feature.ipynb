{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "# import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing#, decomposition, model_selection, metrics, pipeline\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106445, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26610, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "# sample = pd.read_csv('../input/sample_submission.csv')\n",
    "print(train.shape)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4235, 4238, 4265, 8749, 8756, 8818, 18393, 18427, 19020, 25067, 27378, 27768, 31485, 31660, 31671, 31738, 40951, 41030, 41129, 41136, 41152, 41171, 41275, 41586, 42016, 45758, 45759, 45760, 45761, 50687, 51670, 51710, 52231, 52347, 52943, 66800, 70787, 78766, 92399, 92637, 94982, 97814, 98045, 101651, 101850, 101862, 102065, 102117, 102203]\n"
     ]
    }
   ],
   "source": [
    "# delete document with empty content\n",
    "rm_idx = []\n",
    "for i in range(len(test)):\n",
    "    if len(test.Text[i]) <= 20:\n",
    "        print(test.Text[i,])\n",
    "        rm_idx.append(i)\n",
    "\n",
    "print(rm_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917494it [02:21, 13571.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index = {}\n",
    "f = open('./glove.42B.300d.txt', encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WordNet Lemmatizer from nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer() \n",
    "lines_with_lemmas=[] #stop words contain the set of stop words \n",
    "for line in lines: \n",
    "temp_line=[] \n",
    "for word in lines: \n",
    "    temp_line.append (wordnet_lemmatizer.lemmatize(word)) \n",
    "    string=’ ‘ \n",
    "    lines_with_lemmas.append(string.join(temp_line)) \n",
    "    lines=lines_with_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a scaled vector for an entire document\n",
    "def doc2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words) # tokenize the doc\n",
    "    words = [w for w in words if not w in stop_words] # remove stop words\n",
    "    words = [w for w in words if w.isalpha()] # remove numbers\n",
    "\n",
    "    # create an array excluding stopwords and numbers\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "\n",
    "    # for error document, convert into a 300d vector of 0\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "        \n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 106445/106445 [03:14<00:00, 548.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 26610/26610 [00:50<00:00, 524.76it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain = train.Text.values\n",
    "ytrain = train.label\n",
    "xtest = test.Text.values\n",
    "\n",
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xtest_glove = [sent2vec(x) for x in tqdm(xtest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_colname = []\n",
    "test_colname = []\n",
    "for i in range(300):\n",
    "    name = \"v\" + str(i)\n",
    "    train_colname.append(name)\n",
    "    test_colname.append(name)\n",
    "# print(train_colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_output = pd.DataFrame.from_records(xtrain_glove)\n",
    "test_output = pd.DataFrame.from_records(xtest_glove)\n",
    "\n",
    "train_output.columns = train_colname\n",
    "test_output.columns = test_colname\n",
    "\n",
    "# train_output['id'] = train.ID\n",
    "train_output['label'] = ytrain\n",
    "\n",
    "# test_output['id'] = test.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106445, 301)\n",
      "(26610, 300)\n"
     ]
    }
   ],
   "source": [
    "# train_output = train_output[new_traincol]\n",
    "# test_output = test_output[new_testcol]\n",
    "print(train_output.shape)\n",
    "print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output.to_csv('./train_glove_final.csv',index=False)\n",
    "test_output.to_csv('./test_glove_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
